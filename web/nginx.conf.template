server {
  listen 80;

  root /usr/share/nginx/html;
  index index.html;

  location / {
    try_files $uri $uri/ /index.html;
  }

  # OpenAI-like
  location /v1/ {
    proxy_pass http://${UPSTREAM_HOST}:${UPSTREAM_PORT}/v1/;
    proxy_http_version 1.1;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
  }

  # Legacy llama.cpp
  location /completion {
    proxy_pass http://${UPSTREAM_HOST}:${UPSTREAM_PORT}/completion;
    proxy_http_version 1.1;
  }

  location /health {
    return 200 "ok\n";
    add_header Content-Type text/plain;
  }
}
