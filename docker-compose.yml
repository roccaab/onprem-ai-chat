services:
  llama:
    build:
      context: ./llama
    image: onprem-ai-chat-llama:latest
    container_name: llama
    restart: unless-stopped
    environment:
      LLAMA_HOST: "0.0.0.0"
      LLAMA_PORT: "8080"
      CTX: "${CTX:-2048}"
      THREADS: "${THREADS:-4}"
      MODEL_DIR: "/var/models"
      MODEL_FILE: "${MODEL_FILE:-smollm3/SmolLM3-3B-Q4_K_M.gguf}"
      MODEL_URL: "${MODEL_URL:-}"
    volumes:
      - ./models:/var/models
    ports:
      - "${LLAMA_HOST_PORT:-8080}:8080"

  web:
    build:
      context: ./web
    image: onprem-ai-chat-web:latest
    container_name: ai-web
    restart: unless-stopped
    depends_on:
      - llama
    environment:
      UPSTREAM_HOSTPORT: "llama:8080"
      BASIC_AUTH: "${BASIC_AUTH:-0}"
      BASIC_AUTH_USER: "${BASIC_AUTH_USER:-admin}"
      BASIC_AUTH_PASS: "${BASIC_AUTH_PASS:-admin}"
    ports:
      - "${WEB_HOST_PORT:-8089}:80"
